{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Sales Data Analysis Report\"\n",
        "author: \"Pham Quang Huy\"\n",
        "date: \"2024-10-31\"\n",
        "# format: pdf\n",
        "format: pdf\n",
        "#    html: \n",
        "#      code-fold: True\n",
        "jupyter: python3\n",
        "reference-doc: bib-latex\n",
        "---\n",
        "\n",
        "\n",
        "# Sales Data Analysis Report\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "My individual reports keeps track the instruction video of [\"Solving real world data science tasks with Python Pandas!\"](https://github.com/KeithGalli/Pandas-Data-Science-Tasks.git) by [Keith Galli](https://youtu.be/eMOA1pPVUc4?si=-UXWLdSFktCLWSh5). Through power of [Quarto](https://quarto.org/docs/visual-editor/vscode/), I demonstrate the final work of a story telling report to show practices and engage to the audience in a seamless way.\n",
        "\n",
        "In this 'Data Sale Analysis', I use the basics understand from light exploratory data skills in data science practices (implemented Python Pandas and Matlotlib) to complete the walk through instruction in one Jupyter notebook file. The purpose is to get acquainted with **multiple files** of records by months (*./Sales_Data*) and integrating them in one **.csv** file. By learning based on doing, I use my critical thinking and design a notebook to represent the significant insights while providing data visualization to make impacts to the electric stores business.\n",
        "\n",
        "## 2. Data Overview\n",
        "\n",
        "In the competitive world of retail, data holds the key to understanding customer demands and optimizing sales strategies. Many information from data are critical for business decision-making such as trends of sales through periods of time, significant selling programs on specific products.\n",
        "\n",
        "The data nature witness the reverse relation between the 'Sale Quantiy' and 'Price Each', which makes the markets consumer more items like accessories rather than huge investment like laptops, electrics house holds. The relations of low-price product to their large proportion sales amount suggests the demonstration methods by comparing average price.\n",
        "\n",
        "![Figure 1: Subplot of price each items over the quantity of products in total](/Output/PriceQuantityComparision.png){fig-alt=\"Bar chart showing the number of orders per product with an overlaid line plot of the average price for each product. The green bars represent the quantity ordered, while the blue line indicates the average price in dollars. This visualization helps identify popular items and their price points, showing that lower-priced items like batteries have higher sales volumes, whereas premium products like MacBook Pro have fewer orders.\"}\n",
        "\n",
        "There are exceptions in this chart is the office products like MacBook Pro and ThinkPad, which reasonably explains the other impacts of customer demands or the investment cycle into new products. The example of uncover insights from sales data inspired by Keith Galli's tutorial is essentials of uncovering insights including peak periods, best-selling products, and potential strategies to optimize sales.\n",
        "\n",
        "### a/ Data Description\n",
        "\n",
        "The data sources depict set of data in 12 months of electrical stores named \"Keith's SuperDuper cool electronics\". The folder 'data' includes 12 separated `.csv` files for which present the sales data at different stores in a month in that year.\n",
        "\n",
        "### b/ Data Loading\n",
        "\n",
        "In the video, the initial data integration created a one unified `.csv` file containing 12 sales data collections by various months in a year. The merging data from each month in sales data implements read files end with `.csv`. The reason was provided by the author/youtuber is to create an independent place have abundant data for analyzing in details.\n",
        "\n",
        "One more important practice is to `import` very handling libraries.\n"
      ],
      "id": "24b440a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path = \"./Sales_Data\"\n",
        "files = [file for file in os.listdir(path) if file.endswith('.csv')]\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file in files:\n",
        "    file_read = pd.read_csv(path + \"/\" + file)\n",
        "    all_data = pd.concat([all_data, file_read])\n",
        "\n",
        "all_data.to_csv(\"all_data.csv\", index=False)"
      ],
      "id": "f96b4083",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis and Insights\n",
        "\n",
        "`all_data.csv` save amount of 12 files data set is now assigned to variable `df`. From this steps, all methods of exploration date (e.g. `.describe()`, `.info()`, ...) are implemented with following intepretation.\n"
      ],
      "id": "7da4b313"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(\"all_data.csv\")\n",
        "df.info()\n",
        "df.describe()\n",
        "print(df.isnull().sum())"
      ],
      "id": "7b162ace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{\\<RangeIndex: 186850 entries, 0 to 186849 Data columns (total 6 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 Order ID 186305 non-null object 1 Product 186305 non-null object 2 Quantity Ordered 186305 non-null object 3 Price Each 186305 non-null object 4 Order Date 186305 non-null object 5 Purchase Address 186305 non-null object dtypes: object(6) memory usage: 8.6+ MB Order ID 545 Product 545 Quantity Ordered 545 Price Each 545 Order Date 545 Purchase Address 545 dtype: int64\\>}}\n",
        "\n",
        "Data Frame (`df`) used for analysis compounds of 6 columns. The `Order Date` column present date and time. This data columns could help define the months for computation other combination of quantitative information. The 'Product' column contains existing name products ordered per purchase, which could be counted as sum methods to gather attribute of `Price Each`, `Quantity`.\n",
        "\n",
        "`Purchase Address` indicates delivery good destination. The integration of details show city, state, geographical identification. One the quote of purchase is recorded, the column `Order ID` assigned one code for the buyer order. In video, Keith has implemented the data count distinct for each `Order ID` to get the combination of good come together.\n",
        "\n",
        "| Column Name | Description |\n",
        "|----------------------|--------------------------------------------------|\n",
        "| `Order Date` | Represents the date and time of each order. Useful for extracting information like month, day, and hour. |\n",
        "| `Product` | Contains the name of products ordered per purchase, allowing for counts and summaries of each product. |\n",
        "| `Quantity Ordered` | Shows the quantity of each product ordered, used to compute total quantities and sales per product. |\n",
        "| `Price Each` | Indicates the price of each product at the time of purchase, used for calculating total sales. |\n",
        "| `Purchase Address` | Specifies the delivery address, including city, state, and other geographical identifiers. Useful for analyzing sales distribution by location. |\n",
        "| `Order ID` | Unique identifier assigned to each order, which allows counting distinct orders and analyzing product combinations bought together. |\n",
        "\n",
        "### a/ Preprocessing Data\n",
        "\n",
        "The data summary from the primary data interpretation shows shows no missing data from the six columns including 04 categorical data (OrderID, Product, Order Date, Purchase Address), 02 numerical data (Quantity Ordered, Price Each). Otherwise, the nulls values presents all over 6 columns and takes small proportion of values (0.2%).\n",
        "\n",
        "The priority is to find rows of NAN values rows and watch over all numeric data columns. The visualization shows below.\n"
      ],
      "id": "093fb08c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Visualize missing values\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "if df.isnull().values.any(): \n",
        "    numeric_df = df.apply(pd.to_numeric errors='coerce')\n",
        "    sns.heatmap(numeric_df\\[numeric_df.isnull().any(axis=1)\\], cbar=False, cmap='viridis')\n",
        "    else:\n",
        "        print(\"No missing values to visualize.\")"
      ],
      "id": "37944025",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Figure 2: Heatmap Visualizing Missing Values in Dataset](/Output/heatmapNaN.png){fig-alt=\"The heatmap illustrates missing values across different columns in the dataset. Columns with missing data are highlighted, while those without missing data remain unmarked. Here, the `Product`, `Quantity Ordered`, `Price Each`, `Order Date`, and `Purchase Address` columns contain missing values, as represented by the colored bars. The `Order ID` column has no missing values, shown by the absence of gaps.\" fig-align=\"center.\"}\n"
      ],
      "id": "fe9c1424"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "null_data = df[df.isnull().any(axis=1)]\n",
        "null_data.head()"
      ],
      "id": "9c865cff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div>\n",
        "\n",
        "|      | Order ID | Product | Quantity Ordered | Price Each | Order Date | Purchase Address |\n",
        "|----------|----------|----------|----------|----------|----------|----------|\n",
        "| 264  | NaN      | NaN     | NaN              | NaN        | NaN        | NaN              |\n",
        "| 648  | NaN      | NaN     | NaN              | NaN        | NaN        | NaN              |\n",
        "| 680  | NaN      | NaN     | NaN              | NaN        | NaN        | NaN              |\n",
        "| 1385 | NaN      | NaN     | NaN              | NaN        | NaN        | NaN              |\n",
        "| 1495 | NaN      | NaN     | NaN              | NaN        | NaN        | NaN              |\n",
        "\n",
        "</div>\n",
        "\n",
        "### b/ Clean Up the Data\n",
        "\n",
        "\\``{python} df = df.dropna(how='all') df.head()`\n",
        "\n",
        "<div>\n",
        "\n",
        "|   | Order ID | Product | Quantity Ordered | Price Each | Order Date | Purchase Address |\n",
        "|----------|----------|----------|----------|----------|----------|----------|\n",
        "| 0 | 295665 | Macbook Pro Laptop | 1 | 1700 | 12/30/19 00:01 | 136 Church St, New York City, NY 10001 |\n",
        "| 1 | 295666 | LG Washing Machine | 1 | 600.0 | 12/29/19 07:03 | 562 2nd St, New York City, NY 10001 |\n",
        "| 2 | 295667 | USB-C Charging Cable | 1 | 11.95 | 12/12/19 18:21 | 277 Main St, New York City, NY 10001 |\n",
        "| 3 | 295668 | 27in FHD Monitor | 1 | 149.99 | 12/22/19 15:13 | 410 6th St, San Francisco, CA 94016 |\n",
        "| 4 | 295669 | USB-C Charging Cable | 1 | 11.95 | 12/18/19 12:38 | 43 Hill St, Atlanta, GA 30301 |\n",
        "\n",
        "</div>\n",
        "\n",
        "After dropping rows with all null values, we proceed with the following steps to ensure each column has the correct data type, particularly focusing on Order Date, Quantity Ordered, and Price Each.\n",
        "\n",
        "#### Steps for Data Cleaning and Extract Data\n",
        "\n",
        "1.  **Delete Remaining NaN Values:** Rows with NaN values in critical columns, like `Quantity Ordered` and `Price Each`, can skew our calculations. Here, we delete any rows that still contain NaN values.\n",
        "\n",
        "    `df = df.dropna()`\n",
        "\n",
        "2.  **Format the 'Order Date' Column:** The `Order Date` column contains date and time information, which we need in a consistent datetime format. Some rows may have text instead of dates, so we filter those out first.\n"
      ],
      "id": "22b2cf67"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "# Remove rows with text in 'Order Date'\n",
        "df = df[df['Order Date'].str[0:2] != 'Or']\n",
        "\n",
        "# Convert 'Order Date' to datetime\n",
        "df['Order Date'] = pd.to_datetime(df['Order Date'])"
      ],
      "id": "bcc3ee06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.  **Convert Columns to Correct Data Types:** Ensure `Quantity Ordered` and `Price Each` are numeric, as they may currently be strings due to initial data loading.\n"
      ],
      "id": "700c429b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "# Convert to numeric\n",
        "df[['Quantity Ordered', 'Price Each']] = df[['Quantity Ordered', 'Price Each']].apply(pd.to_numeric, errors='coerce')"
      ],
      "id": "3398c98d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.  **Extract Month from 'Order Date':** We can extract the month from the `Order Date` column to analyze monthly sales patterns.\n"
      ],
      "id": "592a3f11"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "df['Month'] = df['Order Date'].dt.month"
      ],
      "id": "31efc9aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.  **Create a City Column:** The `Purchase Address` column contains city and state information, which we can extract for location-based analysis. Using functions allows flexibility for complex address formats.\n"
      ],
      "id": "389b1163"
    },
    {
      "cell_type": "code",
      "metadata": {
        "md-indent": "    "
      },
      "source": [
        "def get_city(address):\n",
        "    return address.split(',')[1].strip()\n",
        "\n",
        "def get_state(address):\n",
        "    return address.split(',')[2].split(' ')[1]\n",
        "\n",
        "df['City'] = df['Purchase Address'].apply(lambda x: f\"{get_city(x)} ({get_state(x)})\")"
      ],
      "id": "beeb7ce2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analysis and Insights\n",
        "\n",
        "With a clean dataset, we’re ready to perform deeper analysis to uncover patterns and insights. The following sections could cover key insights, such as peak sales months, popular products, and sales distributions across different cities.\n",
        "\n",
        "### a/ **What is the best months for sales?**\n",
        "\n",
        "The question as for analysis on 'Sales' grouped by months. By creating a 'Sales' columns we report products of 'Quantity' and 'Price Each'.\n"
      ],
      "id": "e71c4f8d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['Sales'] = df['Quantity Ordered'] * df['Price Each']"
      ],
      "id": "e358ade1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now can choose the columns to calculate base on new information of 'Sales' column. For monthly sales, we choose the 'Month' and then mathematically group `sum` 'Sales' value by it. Here is the example for query and draw charts columns.\n"
      ],
      "id": "e2b5056e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "month_sales = df.groupby('Month')['Sales'].sum()\n",
        "month_sales.plot(kind='bar')\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.ylabel('Sales in USD')\n",
        "plt.xlabel('Month')\n",
        "plt.title('Total Sales by Month')\n",
        "plt.show()"
      ],
      "id": "a2a31e8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"/Output/Total_Sales_ByMonths.png\" alt=\"Blue columns sorted by months on x-axis and the y-axis show the Sales in USD. The Sales increase by the first 4 months and evenly decrease the total sales still it first soaks on October. The next months show the down fall in total sales then rocket to highest sales of the year at nearly 5 million us dollar.\"/>*Figure 3: Total Sales By Month*\n",
        "\n",
        "The sales data over the period from **January to December** reveals notable trends in overall sales performance showing **a steady increase in sales** over the year with several **seasonal spikes** in specific months. This pattern suggests growing customer engagement and effective sales strategies, especially during peak periods.\n",
        "\n",
        "**Monthly Higlights** Certain months stand out for their impact on sales performance:\n",
        "\n",
        "-   **December** recorded the **highest sales volume**, likely due to the holiday season and year-end promotions, capturing increased consumer spending.\n",
        "\n",
        "-   **October** saw a significant boost, possibly driven by a targeted sale program or promotion.\n",
        "\n",
        "-   April to November showed a gradual decline, indicating a stabilization in demand. This may suggest a shift from high initial interest and purchases to more spontaneous or needs-based buying, as customers adjusted after initial seasonal spikes.\n",
        "\n",
        "**Key Observations**\n",
        "\n",
        "-   **Peak Sales in December:** The highest sales in December highlights a successful capitalizing on holiday shopping trends. This aligns with typical consumer behavior during this time, where year-end promotions boost spending.\n",
        "-   **October Sale Spike:** A strong increase in October suggests that promotions were effective, attracting consumers looking for discounts ahead of a main season in year.\n",
        "-   **Lower Mid-year Sales** The dip in sales from mid-year could indicate a reduction in consumer spending after an initial surge in the first quarter. This may reflect a natural leveling off, as the early months often see heightened consumer enthusiasm, which tapers as customers become more selective or budget-conscious later in the year.\n",
        "\n",
        "### b/ **What city sold the most product?**\n",
        "\n",
        "For cities, we could focus on deciding group of 'Sales' by 'Cities'. This analysis could presents the sales values and its performance through city.\n"
      ],
      "id": "86414a56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "city_sales = df.groupby('City')['Sales'].sum().sort_values()\n",
        "city_sales.plot(kind='bar')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.ylabel('Sales in USD')\n",
        "plt.xlabel('City')\n",
        "plt.title('Total Sales by City')\n",
        "plt.show()"
      ],
      "id": "3808243a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "<img src=\"/Output/Total_Sales_byCity.png\" alt=\"Total Sales By City\"/> *Figure 4: Total Sales By City*\n",
        "\n",
        "**City Comparisons (Sales By City)** Analyzing the sales data across different cities reveals significant variations in performance. By grouping the data by 'City' and calculating the total sales, we can compare the sales volume to identify the highest and lowest performers.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "468da902"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/usr/local/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}